{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis using SparkSession "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Session will provide us with RDD's which are very similar to DataFrames of pandas.\n",
    "\n",
    "In this notebook we will load a csv file and find the count of customers based on cities.\n",
    "\n",
    "In the process above, we shall look at some main functions of the dataframes of spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Java Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark is only compatible with Java8 and hence specify its path\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk1.8.0_202'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = pyspark.SparkConf().set('spark.driver.host', '127.0.0.1')\n",
    "\n",
    "sc = pyspark.SparkContext(master='local', appName='myAppName', conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file into a dataframe and make first entry as header\n",
    "df = spark.read.option(\"header\", \"true\").csv('group_by_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, null_val: string, country: string, region: string, state: string, city: string, date_value: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the info about df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------+------+------------+---------+--------------------+\n",
      "|          id|null_val|country|region|       state|     city|          date_value|\n",
      "+------------+--------+-------+------+------------+---------+--------------------+\n",
      "|Cust_2337980|    null|  India| south|   karnataka|Bangalore|2020-01-10 15:10:...|\n",
      "|Cust_5132227|    null|  India| north|     Haryana|   Rohtak|2020-01-10 15:10:...|\n",
      "|Cust_3758463|    null|  India|  west|     Gujarat|Ahmedabad|2020-01-10 15:10:...|\n",
      "|Cust_5782791|    null|  India| north|UttarPradesh|   Kanpur|2020-01-10 15:10:...|\n",
      "|Cust_6471658|    null|  India| north|     Haryana|   Rohtak|2020-01-10 15:10:...|\n",
      "|Cust_1223463|    null|  India|  east|    Jharkand|   Ranchi|2020-01-10 15:10:...|\n",
      "|Cust_2890272|    null|  India|  east|    Jharkand|   Ranchi|2020-01-10 15:10:...|\n",
      "|Cust_7912873|    null|  India| south|   karnataka|   Mysore|2020-01-10 15:10:...|\n",
      "|Cust_6593030|    null|  India| north|UttarPradesh|  Lucknow|2020-01-10 15:10:...|\n",
      "|Cust_6783648|    null|  India| north|     Haryana|   Rohtak|2020-01-10 15:10:...|\n",
      "|Cust_6527109|    null|  India|  west|     Gujarat| Vadodara|2020-01-10 15:10:...|\n",
      "|Cust_3688463|    null|  India| north|UttarPradesh|   Kanpur|2020-01-10 15:10:...|\n",
      "|Cust_5761822|    null|  India| south|  tamil nadu|  Chennai|2020-01-10 15:10:...|\n",
      "|Cust_3934433|    null|  India| south|   karnataka|Bangalore|2020-01-10 15:10:...|\n",
      "|Cust_1277390|    null|  India| south|   karnataka|   Mysore|2020-01-10 15:10:...|\n",
      "|Cust_3624918|    null|  India|  west| Maharashtra|     Pune|2020-01-10 15:10:...|\n",
      "|Cust_9497154|    null|  India|  west|     Gujarat| Vadodara|2020-01-10 15:10:...|\n",
      "|Cust_2620184|    null|  India|  west| Maharashtra|     Pune|2020-01-10 15:10:...|\n",
      "|Cust_4292870|    null|  India| south|  tamil nadu| Madhurai|2020-01-10 15:10:...|\n",
      "|Cust_3537713|    null|  India| north|     Haryana|   Rohtak|2020-01-10 15:10:...|\n",
      "+------------+--------+-------+------+------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get top 20 rows to check how the data looks like\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- remarks: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- date_value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get information about all the columns in the dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City related analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of SparkSession is its similarity with SQL\n",
    "\n",
    "the below are the major commands\n",
    "1. select --> this is similar to Select in SQL, can be used to pick certain columns from the dataframe\n",
    "2. filter --> this is similar to where clause in SQL, can be used to filter out based on condition\n",
    "3. groupBy--> exaclty same as SQL Group-By clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(city='Bangalore'),\n",
       " Row(city='Rohtak'),\n",
       " Row(city='Ahmedabad'),\n",
       " Row(city='Kanpur')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the top 4 rows and get only city values\n",
    "df.select('city').head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get states and countries in seperate df\n",
    "country_state_map=df.select(df['country'],df['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|country|       state|\n",
      "+-------+------------+\n",
      "|  India|   karnataka|\n",
      "|  India|     Haryana|\n",
      "|  India|     Gujarat|\n",
      "|  India|UttarPradesh|\n",
      "|  India|     Haryana|\n",
      "|  India|    Jharkand|\n",
      "|  India|    Jharkand|\n",
      "|  India|   karnataka|\n",
      "|  India|UttarPradesh|\n",
      "|  India|     Haryana|\n",
      "|  India|     Gujarat|\n",
      "|  India|UttarPradesh|\n",
      "|  India|  tamil nadu|\n",
      "|  India|   karnataka|\n",
      "|  India|   karnataka|\n",
      "|  India| Maharashtra|\n",
      "|  India|     Gujarat|\n",
      "|  India| Maharashtra|\n",
      "|  India|  tamil nadu|\n",
      "|  India|     Haryana|\n",
      "+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the results\n",
    "country_state_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='Cust_2337980', remarks=None, country='India', region='south', state='karnataka', city='Bangalore', date_value='2020-01-10 15:10:38.562000'),\n",
       " Row(id='Cust_3934433', remarks=None, country='India', region='south', state='karnataka', city='Bangalore', date_value='2020-01-10 15:10:38.562000'),\n",
       " Row(id='Cust_9294147', remarks=None, country='India', region='south', state='karnataka', city='Bangalore', date_value='2020-01-10 15:10:38.562000'),\n",
       " Row(id='Cust_2772376', remarks=None, country='India', region='south', state='karnataka', city='Bangalore', date_value='2020-01-10 15:10:38.562000'),\n",
       " Row(id='Cust_4252429', remarks=None, country='India', region='south', state='karnataka', city='Bangalore', date_value='2020-01-10 15:10:38.562000')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the top 5 rows where city is \"Bangaloreabs\"\n",
    "df.filter(df['city']=='Bangalore').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      city|count|\n",
      "+----------+-----+\n",
      "| Bangalore| 6153|\n",
      "|    Mysore| 6255|\n",
      "|  Vadodara| 6307|\n",
      "|   Lucknow| 6258|\n",
      "|   Chennai| 6127|\n",
      "|    Ranchi| 6250|\n",
      "|Jamshedpur| 6266|\n",
      "|    Mumbai| 6150|\n",
      "| Ahmedabad| 6216|\n",
      "|   Kolkata| 6204|\n",
      "|    Rohtak| 6403|\n",
      "|   Gurgaon| 6205|\n",
      "|      Pune| 6327|\n",
      "|    Kanpur| 6323|\n",
      "|Darjeeling| 6285|\n",
      "|  Madhurai| 6271|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get count of rows per each cutomer and view results\n",
    "df.groupBy(df['city']).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL queries in SPARK\n",
    "\n",
    "We can write SQL queries and execute them in spark directly without using HIVE,\n",
    "\n",
    "This can be achived by using sparksql.\n",
    "\n",
    "We will make a view from the dataframe and then run the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('cust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts_df=spark.sql('select city,count(1) from cust group by city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      city|count(1)|\n",
      "+----------+--------+\n",
      "| Bangalore|    6153|\n",
      "|    Mysore|    6255|\n",
      "|  Vadodara|    6307|\n",
      "|   Lucknow|    6258|\n",
      "|   Chennai|    6127|\n",
      "|    Ranchi|    6250|\n",
      "|Jamshedpur|    6266|\n",
      "|    Mumbai|    6150|\n",
      "| Ahmedabad|    6216|\n",
      "|   Kolkata|    6204|\n",
      "|    Rohtak|    6403|\n",
      "|   Gurgaon|    6205|\n",
      "|      Pune|    6327|\n",
      "|    Kanpur|    6323|\n",
      "|Darjeeling|    6285|\n",
      "|  Madhurai|    6271|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_counts_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_city_df=spark.sql('select distinct city from cust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      city|\n",
      "+----------+\n",
      "| Bangalore|\n",
      "|    Mysore|\n",
      "|  Vadodara|\n",
      "|   Lucknow|\n",
      "|   Chennai|\n",
      "|    Ranchi|\n",
      "|Jamshedpur|\n",
      "|    Mumbai|\n",
      "| Ahmedabad|\n",
      "|   Kolkata|\n",
      "|    Rohtak|\n",
      "|   Gurgaon|\n",
      "|      Pune|\n",
      "|    Kanpur|\n",
      "|Darjeeling|\n",
      "|  Madhurai|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_city_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halt SparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SparkSession will be used to make sql like operations on DF objects\n",
    "2. We have majorly Select,Filter and GroupBY\n",
    "3. createOrReplaceTempView can be used to make a view on a df\n",
    "4. using spark.sql we can run sql queries on views that was created from df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
